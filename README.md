# Text-Generation-Model---LSTM-GPT-Implementation
LSTMs (RNNs) generate text sequentially, learning long-range dependencies via internal gates for next-word prediction. GPT (Transformer) uses self-attention for parallel processing and vast pre-training, enabling highly coherent, context-aware text generation by predicting the next token, excelling in scalability and quality.
<img width="767" height="605" alt="image" src="https://github.com/user-attachments/assets/75bbe288-6ccb-4299-a3e3-7d2a21a53171" />
<img width="1035" height="736" alt="image" src="https://github.com/user-attachments/assets/66aca869-7a52-429c-8951-9840d9700ad9" />
<img width="376" height="608" alt="image" src="https://github.com/user-attachments/assets/49ff1233-481d-4c31-a438-4952dba3fa21" />
<img width="741" height="372" alt="image" src="https://github.com/user-attachments/assets/1d41a71a-08d7-4c0a-9850-e64a70da14a1" />
<img width="952" height="632" alt="image" src="https://github.com/user-attachments/assets/79f7adfe-503a-4dff-a017-55a6dbc4b4ff" />
<img width="781" height="588" alt="image" src="https://github.com/user-attachments/assets/cf565bda-70c5-4ef6-9603-59df563d3f0c" />
<img width="1421" height="508" alt="image" src="https://github.com/user-attachments/assets/2eb81231-a4ba-4fa8-bc4c-625fb12bfa01" />
