# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19DBTY-3r1oKP_NqmICcKMlfykXqxsxzO
"""

#!/usr/bin/env python3
"""
Text Generation Model - LSTM & GPT Implementation
CODTECH Internship Project

This notebook demonstrates text generation using both LSTM and Transformer-based models
to generate coherent paragraphs on specific topics.

Author: CODTECH Intern
Date: 2025
"""

import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, Input, MultiHeadAttention, LayerNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
import re
import pickle
import os
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# Set style for plots
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

class TextPreprocessor:
    """
    Text preprocessing class for cleaning and tokenizing text data
    """

    def __init__(self):
        self.vocab_to_int = {}
        self.int_to_vocab = {}
        self.vocab_size = 0
        self.sequence_length = 50  # Reduced from 100 to 50

    def clean_text(self, text):
        """
        Clean and normalize text

        Args:
            text (str): Raw text

        Returns:
            str: Cleaned text
        """
        # Convert to lowercase
        text = text.lower()

        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text)

        # Remove special characters but keep punctuation
        text = re.sub(r'[^\w\s.,!?;:\'"()-]', '', text)

        # Remove extra spaces
        text = text.strip()

        return text

    def create_vocabulary(self, texts):
        """
        Create vocabulary from text corpus

        Args:
            texts (list): List of text strings
        """
        # Combine all texts
        combined_text = ' '.join(texts)

        # Clean the text
        cleaned_text = self.clean_text(combined_text)

        # Tokenize
        words = cleaned_text.split()

        # Count word frequencies
        word_counts = Counter(words)

        # Create vocabulary (keep words that appear at least 2 times)
        vocab_words = [word for word, count in word_counts.items() if count >= 2]

        # Add special tokens
        vocab_words = ['<PAD>', '<UNK>', '<START>', '<END>'] + vocab_words

        # Create mappings
        self.vocab_to_int = {word: i for i, word in enumerate(vocab_words)}
        self.int_to_vocab = {i: word for i, word in enumerate(vocab_words)}
        self.vocab_size = len(vocab_words)

        print(f"Vocabulary size: {self.vocab_size}")

    def text_to_sequences(self, texts):
        """
        Convert texts to sequences of integers

        Args:
            texts (list): List of text strings

        Returns:
            list: List of integer sequences
        """
        sequences = []

        for text in texts:
            cleaned_text = self.clean_text(text)
            words = cleaned_text.split()

            # Convert words to integers
            sequence = [self.vocab_to_int.get(word, self.vocab_to_int['<UNK>']) for word in words]

            # Add start and end tokens
            sequence = [self.vocab_to_int['<START>']] + sequence + [self.vocab_to_int['<END>']]

            sequences.append(sequence)

        return sequences

    def create_training_data(self, sequences):
        """
        Create training data from sequences

        Args:
            sequences (list): List of integer sequences

        Returns:
            tuple: (X, y) training data
        """
        X, y = [], []

        for sequence in sequences:
            # Only process sequences longer than sequence_length + 1
            if len(sequence) > self.sequence_length:
                for i in range(len(sequence) - self.sequence_length):
                    X.append(sequence[i:i + self.sequence_length])
                    y.append(sequence[i + self.sequence_length])

        print(f"Created {len(X)} training samples from {len(sequences)} sequences")
        return np.array(X), np.array(y)

class LSTMTextGenerator:
    """
    LSTM-based text generation model
    """

    def __init__(self, vocab_size, embedding_dim=256, lstm_units=512, sequence_length=50):
        """
        Initialize LSTM text generator

        Args:
            vocab_size (int): Size of vocabulary
            embedding_dim (int): Embedding dimension
            lstm_units (int): Number of LSTM units
            sequence_length (int): Length of input sequences
        """
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.lstm_units = lstm_units
        self.sequence_length = sequence_length
        self.model = None

    def build_model(self):
        """
        Build LSTM model architecture
        """
        self.model = Sequential([
            Embedding(self.vocab_size, self.embedding_dim, input_length=self.sequence_length),
            LSTM(self.lstm_units, return_sequences=True, dropout=0.3, recurrent_dropout=0.3),
            LSTM(self.lstm_units, dropout=0.3, recurrent_dropout=0.3),
            Dense(self.vocab_size // 2, activation='relu'),
            Dropout(0.5),
            Dense(self.vocab_size, activation='softmax')
        ])

        self.model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )

        print("LSTM Model Architecture:")
        self.model.summary()

    def train(self, X_train, y_train, X_val, y_val, epochs=50, batch_size=128):
        """
        Train the LSTM model

        Args:
            X_train, y_train: Training data
            X_val, y_val: Validation data
            epochs (int): Number of training epochs
            batch_size (int): Batch size

        Returns:
            History: Training history
        """
        callbacks = [
            EarlyStopping(patience=10, restore_best_weights=True),
            ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6)
        ]

        history = self.model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )

        return history

    def generate_text(self, seed_text, preprocessor, max_length=200, temperature=0.8):
        """
        Generate text using the trained model

        Args:
            seed_text (str): Starting text
            preprocessor: Text preprocessor object
            max_length (int): Maximum length of generated text
            temperature (float): Sampling temperature

        Returns:
            str: Generated text
        """
        if self.model is None:
            raise ValueError("Model not built or trained!")

        # Preprocess seed text
        cleaned_seed = preprocessor.clean_text(seed_text)
        words = cleaned_seed.split()

        # Convert to sequence
        sequence = [preprocessor.vocab_to_int.get(word, preprocessor.vocab_to_int['<UNK>']) for word in words]

        # Ensure sequence has the right length
        if len(sequence) < self.sequence_length:
            sequence = [preprocessor.vocab_to_int['<PAD>']] * (self.sequence_length - len(sequence)) + sequence
        else:
            sequence = sequence[-self.sequence_length:]

        generated_words = words.copy()

        for _ in range(max_length):
            # Predict next word
            x = np.array([sequence])
            predictions = self.model.predict(x, verbose=0)[0]

            # Apply temperature
            predictions = np.log(predictions + 1e-8) / temperature
            predictions = np.exp(predictions)
            predictions = predictions / np.sum(predictions)

            # Sample next word
            next_word_idx = np.random.choice(len(predictions), p=predictions)

            # Stop if end token
            if next_word_idx == preprocessor.vocab_to_int['<END>']:
                break

            # Get word and add to generated text
            next_word = preprocessor.int_to_vocab[next_word_idx]
            if next_word not in ['<PAD>', '<UNK>', '<START>']:
                generated_words.append(next_word)

            # Update sequence
            sequence = sequence[1:] + [next_word_idx]

        return ' '.join(generated_words)

class SimpleTransformer:
    """
    Simple Transformer-based text generation model
    """

    def __init__(self, vocab_size, d_model=256, num_heads=8, num_layers=4, sequence_length=50):
        """
        Initialize Transformer text generator

        Args:
            vocab_size (int): Size of vocabulary
            d_model (int): Model dimension
            num_heads (int): Number of attention heads
            num_layers (int): Number of transformer layers
            sequence_length (int): Length of input sequences
        """
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.sequence_length = sequence_length
        self.model = None

    def positional_encoding(self, position, d_model):
        """
        Create positional encoding

        Args:
            position (int): Position
            d_model (int): Model dimension

        Returns:
            tf.Tensor: Positional encoding
        """
        angle_rads = np.arange(position)[:, np.newaxis] / np.power(10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / np.float32(d_model))

        # Apply sin to even indices
        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])

        # Apply cos to odd indices
        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])

        pos_encoding = angle_rads[np.newaxis, ...]

        return tf.cast(pos_encoding, dtype=tf.float32)

    def transformer_block(self, inputs, name_prefix):
        """
        Create a transformer block

        Args:
            inputs: Input tensor
            name_prefix (str): Name prefix for layers

        Returns:
            tf.Tensor: Output tensor
        """
        # Multi-head attention
        attention_output = MultiHeadAttention(
            num_heads=self.num_heads,
            key_dim=self.d_model // self.num_heads,
            name=f"{name_prefix}_attention"
        )(inputs, inputs)

        # Add & Norm
        attention_output = LayerNormalization(name=f"{name_prefix}_ln1")(inputs + attention_output)

        # Feed forward
        ffn_output = Dense(self.d_model * 4, activation='relu', name=f"{name_prefix}_ffn1")(attention_output)
        ffn_output = Dense(self.d_model, name=f"{name_prefix}_ffn2")(ffn_output)

        # Add & Norm
        output = LayerNormalization(name=f"{name_prefix}_ln2")(attention_output + ffn_output)

        return output

    def build_model(self):
        """
        Build Transformer model architecture
        """
        inputs = Input(shape=(self.sequence_length,))

        # Embedding
        embedding = Embedding(self.vocab_size, self.d_model)(inputs)

        # Add positional encoding
        pos_encoding = self.positional_encoding(self.sequence_length, self.d_model)
        embedding = embedding + pos_encoding

        # Transformer blocks
        x = embedding
        for i in range(self.num_layers):
            x = self.transformer_block(x, f"transformer_block_{i}")

        # Final dense layer
        outputs = Dense(self.vocab_size, activation='softmax', name="output_dense")(x[:, -1, :])

        self.model = Model(inputs=inputs, outputs=outputs)

        self.model.compile(
            optimizer=Adam(learning_rate=0.0001),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )

        print("Transformer Model Architecture:")
        self.model.summary()

    def train(self, X_train, y_train, X_val, y_val, epochs=30, batch_size=64):
        """
        Train the Transformer model

        Args:
            X_train, y_train: Training data
            X_val, y_val: Validation data
            epochs (int): Number of training epochs
            batch_size (int): Batch size

        Returns:
            History: Training history
        """
        callbacks = [
            EarlyStopping(patience=8, restore_best_weights=True),
            ReduceLROnPlateau(factor=0.7, patience=4, min_lr=1e-7)
        ]

        history = self.model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )

        return history

    def generate_text(self, seed_text, preprocessor, max_length=200, temperature=0.9):
        """
        Generate text using the trained Transformer model

        Args:
            seed_text (str): Starting text
            preprocessor: Text preprocessor object
            max_length (int): Maximum length of generated text
            temperature (float): Sampling temperature

        Returns:
            str: Generated text
        """
        if self.model is None:
            raise ValueError("Model not built or trained!")

        # Preprocess seed text
        cleaned_seed = preprocessor.clean_text(seed_text)
        words = cleaned_seed.split()

        # Convert to sequence
        sequence = [preprocessor.vocab_to_int.get(word, preprocessor.vocab_to_int['<UNK>']) for word in words]

        # Ensure sequence has the right length
        if len(sequence) < self.sequence_length:
            sequence = [preprocessor.vocab_to_int['<PAD>']] * (self.sequence_length - len(sequence)) + sequence
        else:
            sequence = sequence[-self.sequence_length:]

        generated_words = words.copy()

        for _ in range(max_length):
            # Predict next word
            x = np.array([sequence])
            predictions = self.model.predict(x, verbose=0)[0]

            # Apply temperature
            predictions = np.log(predictions + 1e-8) / temperature
            predictions = np.exp(predictions)
            predictions = predictions / np.sum(predictions)

            # Sample next word
            next_word_idx = np.random.choice(len(predictions), p=predictions)

            # Stop if end token
            if next_word_idx == preprocessor.vocab_to_int['<END>']:
                break

            # Get word and add to generated text
            next_word = preprocessor.int_to_vocab[next_word_idx]
            if next_word not in ['<PAD>', '<UNK>', '<START>']:
                generated_words.append(next_word)

            # Update sequence (sliding window)
            sequence = sequence[1:] + [next_word_idx]

        return ' '.join(generated_words)

def create_sample_dataset():
    """
    Create a sample dataset for training

    Returns:
        list: List of sample texts
    """
    sample_texts = [
        # Technology texts - Extended for better training
        "Artificial intelligence is revolutionizing the way we interact with technology and changing our daily lives. Machine learning algorithms can now process vast amounts of data and identify complex patterns that humans might miss entirely. Deep learning networks are particularly effective at tasks like image recognition, natural language processing, and predictive analytics. These systems continue to evolve and improve with more data and computational power.",

        "The future of computing lies in quantum mechanics and its revolutionary applications. Quantum computers use quantum bits or qubits that can exist in multiple states simultaneously, enabling superposition. This allows them to perform certain calculations exponentially faster than classical computers ever could. Quantum supremacy represents a major milestone in computational science and technology development.",

        "Blockchain technology provides a decentralized approach to data storage and transaction verification systems. Each block contains a cryptographic hash of the previous block, creating an immutable chain of transactions and data records. This revolutionary technology has applications far beyond cryptocurrency, including supply chain management, digital identity verification, smart contracts, and secure voting systems.",

        # Science texts - Extended
        "The human brain contains approximately 86 billion neurons, each connected to thousands of others through complex synaptic networks. These neural networks process information through electrical and chemical signals, enabling consciousness, memory formation, learning, and cognitive functions. Neuroscience research continues to uncover the mysteries of how these biological networks create our thoughts, emotions, and behaviors.",

        "Climate change is one of the most pressing global challenges of our time and requires immediate action. Rising global temperatures are causing polar ice caps to melt rapidly, sea levels to rise dangerously, and weather patterns to become increasingly extreme and unpredictable. Renewable energy sources like solar power, wind energy, and hydroelectric systems offer promising solutions for reducing carbon emissions and combating environmental degradation.",

        "Space exploration continues to push the boundaries of human knowledge and technological capabilities. Missions to Mars are revealing fascinating secrets about the red planet's geology, atmosphere, and potential for past or present life forms. The James Webb Space Telescope is providing unprecedented views of distant galaxies, star formation processes, and the early universe, revolutionizing our understanding of cosmic evolution.",

        # History texts - Extended
        "The Renaissance was a transformative period of cultural rebirth in Europe from the 14th to 17th centuries. Great artists like Leonardo da Vinci and Michelangelo created timeless masterpieces that continue to inspire people today. Scientific discoveries by brilliant minds like Galileo Galilei and Nicolaus Copernicus fundamentally changed our understanding of the universe, Earth's place in it, and the scientific method itself.",

        "World War II was a devastating global conflict that lasted from 1939 to 1945, involving most nations worldwide. It resulted in significant technological advances, including the development of radar technology, jet engines, computer systems, and nuclear technology. The war's aftermath shaped the modern world order, international relations, and technological progress for decades to come.",

        "The Industrial Revolution transformed human society from agricultural to manufacturing-based economies during the 18th and 19th centuries. Steam engines powered factories and revolutionized transportation systems, while new production methods dramatically increased efficiency and output. This period marked the beginning of modern industrial society and mass production techniques that continue to evolve today.",

        # Literature and philosophy - Extended
        "Literature has the remarkable power to transport readers to different worlds, perspectives, and emotional experiences. Great authors like William Shakespeare, Charles Dickens, and Jane Austen created unforgettable characters and compelling stories that continue to resonate with audiences centuries after they were written. Their works explore universal themes of love, loss, ambition, and human nature that remain relevant across cultures and generations.",

        "Philosophy seeks to understand fundamental questions about existence, knowledge, morality, and the meaning of life itself. Ancient philosophers like Plato and Aristotle laid the intellectual groundwork for Western philosophical thought and logical reasoning. Eastern philosophers like Confucius and Lao Tzu offered different perspectives on wisdom, virtue, harmony, and the relationship between individuals and society.",

        "Creative writing involves the careful craft of narratives that engage readers both emotionally and intellectually through storytelling. Authors use sophisticated literary devices like metaphor, symbolism, foreshadowing, and character development to create depth and meaning in their work. The best literature challenges readers to think critically about complex issues while providing entertainment and emotional satisfaction through well-constructed plots and memorable characters.",
    ]

    # Add more varied content for better training
    additional_texts = [
        "Technology companies are investing billions in artificial intelligence research and development. Machine learning models require massive datasets and computational resources to train effectively. Natural language processing has advanced significantly with transformer architectures and attention mechanisms.",

        "Environmental science studies the interactions between physical, chemical, and biological components of our planet. Ecosystem conservation requires understanding complex relationships between species and their habitats. Biodiversity loss threatens the stability of natural systems worldwide.",

        "Historical analysis reveals patterns in human behavior and societal development over time. Archaeological discoveries provide evidence about ancient civilizations and their cultural practices. Understanding the past helps us make better decisions for the future.",

        "Mathematical concepts form the foundation of scientific understanding and technological advancement. Statistics and probability theory enable data analysis and decision making under uncertainty. Calculus describes rates of change and optimization in various fields.",

        "Medical research continues to develop new treatments and therapies for various diseases. Genetic engineering offers potential solutions for inherited disorders and cancer treatment. Personalized medicine tailors treatments to individual patient characteristics and needs.",

        "Educational psychology examines how people learn and retain information effectively. Different learning styles require adapted teaching methods and assessment strategies. Technology integration in classrooms changes traditional educational approaches and student engagement.",

        "Economic systems determine resource allocation and wealth distribution in societies. Market forces influence supply and demand for goods and services. International trade connects global markets but also creates economic dependencies.",

        "Social media platforms have transformed human communication and information sharing patterns. Digital networks enable instant global connectivity but also raise privacy concerns. Online communities form around shared interests and common goals.",

        "Renewable energy technologies are becoming more efficient and cost-effective than fossil fuels. Solar panels and wind turbines generate clean electricity with minimal environmental impact. Energy storage solutions address the intermittent nature of renewable sources.",

        "Space technology enables satellite communications, weather monitoring, and GPS navigation systems. Rocket propulsion systems have evolved from chemical to ion and nuclear thermal propulsion. Mars colonization requires solving challenges in life support, radiation protection, and resource utilization."
    ]

    # Combine and duplicate for larger dataset
    all_texts = sample_texts + additional_texts
    extended_texts = all_texts * 3  # Triple the dataset

    return extended_texts

def plot_training_history(history_lstm, history_transformer):
    """
    Plot training history for both models

    Args:
        history_lstm: LSTM training history
        history_transformer: Transformer training history
    """
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # LSTM Loss
    axes[0, 0].plot(history_lstm.history['loss'], label='Training Loss', linewidth=2)
    axes[0, 0].plot(history_lstm.history['val_loss'], label='Validation Loss', linewidth=2)
    axes[0, 0].set_title('LSTM Model - Loss', fontsize=14, fontweight='bold')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)

    # LSTM Accuracy
    axes[0, 1].plot(history_lstm.history['accuracy'], label='Training Accuracy', linewidth=2)
    axes[0, 1].plot(history_lstm.history['val_accuracy'], label='Validation Accuracy', linewidth=2)
    axes[0, 1].set_title('LSTM Model - Accuracy', fontsize=14, fontweight='bold')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Accuracy')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)

    # Transformer Loss
    axes[1, 0].plot(history_transformer.history['loss'], label='Training Loss', linewidth=2)
    axes[1, 0].plot(history_transformer.history['val_loss'], label='Validation Loss', linewidth=2)
    axes[1, 0].set_title('Transformer Model - Loss', fontsize=14, fontweight='bold')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('Loss')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)

    # Transformer Accuracy
    axes[1, 1].plot(history_transformer.history['accuracy'], label='Training Accuracy', linewidth=2)
    axes[1, 1].plot(history_transformer.history['val_accuracy'], label='Validation Accuracy', linewidth=2)
    axes[1, 1].set_title('Transformer Model - Accuracy', fontsize=14, fontweight='bold')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Accuracy')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

def compare_generations(lstm_model, transformer_model, preprocessor, prompts):
    """
    Compare text generation from both models

    Args:
        lstm_model: Trained LSTM model
        transformer_model: Trained Transformer model
        preprocessor: Text preprocessor
        prompts (list): List of prompts to test
    """
    print("="*80)
    print("TEXT GENERATION COMPARISON")
    print("="*80)

    for i, prompt in enumerate(prompts, 1):
        print(f"\n🎯 PROMPT {i}: {prompt}")
        print("-" * 60)

        # Generate with LSTM
        lstm_text = lstm_model.generate_text(prompt, preprocessor, max_length=150, temperature=0.8)
        print(f"📝 LSTM Generated Text:")
        print(f"{lstm_text}\n")

        # Generate with Transformer
        transformer_text = transformer_model.generate_text(prompt, preprocessor, max_length=150, temperature=0.9)
        print(f"🤖 Transformer Generated Text:")
        print(f"{transformer_text}\n")

        print("=" * 60)

def main():
    """
    Main function to demonstrate text generation models
    """
    print("="*80)
    print("TEXT GENERATION MODEL - CODTECH INTERNSHIP PROJECT")
    print("="*80)

    # Step 1: Create dataset
    print("\n1. Creating Sample Dataset...")
    texts = create_sample_dataset()
    print(f"Dataset created with {len(texts)} text samples")

    # Step 2: Preprocess data
    print("\n2. Preprocessing Text Data...")
    preprocessor = TextPreprocessor()
    preprocessor.create_vocabulary(texts)

    # Convert texts to sequences
    sequences = preprocessor.text_to_sequences(texts)
    print(f"Number of sequences: {len(sequences)}")

    # Check sequence lengths
    seq_lengths = [len(seq) for seq in sequences]
    print(f"Sequence lengths - Min: {min(seq_lengths)}, Max: {max(seq_lengths)}, Avg: {sum(seq_lengths)/len(seq_lengths):.1f}")

    X, y = preprocessor.create_training_data(sequences)

    print(f"Training data shape: X={X.shape}, y={y.shape}")

    # Split data
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
    print(f"Train: {X_train.shape}, Validation: {X_val.shape}")

    # Step 3: Build and train LSTM model
    print("\n3. Building and Training LSTM Model...")
    lstm_model = LSTMTextGenerator(
        vocab_size=preprocessor.vocab_size,
        embedding_dim=128,
        lstm_units=256,
        sequence_length=preprocessor.sequence_length
    )
    lstm_model.build_model()

    print("Training LSTM model...")
    history_lstm = lstm_model.train(X_train, y_train, X_val, y_val, epochs=20, batch_size=64)

    # Step 4: Build and train Transformer model
    print("\n4. Building and Training Transformer Model...")
    transformer_model = SimpleTransformer(
        vocab_size=preprocessor.vocab_size,
        d_model=128,
        num_heads=4,
        num_layers=2,
        sequence_length=preprocessor.sequence_length
    )
    transformer_model.build_model()

    print("Training Transformer model...")
    history_transformer = transformer_model.train(X_train, y_train, X_val, y_val, epochs=15, batch_size=32)

    # Step 5: Visualize training results
    print("\n5. Visualizing Training Results...")
    plot_training_history(history_lstm, history_transformer)

    # Step 6: Generate text with different prompts
    print("\n6. Generating Text with Both Models...")

    test_prompts = [
        "Artificial intelligence",
        "The future of technology",
        "Climate change is",
        "Space exploration reveals",
        "Machine learning algorithms",
        "Quantum computing will"
    ]

    compare_generations(lstm_model, transformer_model, preprocessor, test_prompts)

    # Step 7: Interactive text generation
    print("\n7. Interactive Text Generation Demo")
    print("-" * 50)

    while True:
        user_prompt = input("\nEnter a prompt for text generation (or 'quit' to exit): ").strip()

        if user_prompt.lower() in ['quit', 'exit', 'q']:
            break

        if user_prompt:
            print(f"\n🎯 Your prompt: {user_prompt}")
            print("-" * 40)

            # Generate with both models
            lstm_result = lstm_model.generate_text(user_prompt, preprocessor, max_length=100, temperature=0.8)
            transformer_result = transformer_model.generate_text(user_prompt, preprocessor, max_length=100, temperature=0.9)

            print(f"📝 LSTM Result:\n{lstm_result}\n")
            print(f"🤖 Transformer Result:\n{transformer_result}\n")

    # Step 8: Save models and preprocessor
    print("\n8. Saving Models and Preprocessor...")

    # Save models
    lstm_model.model.save('lstm_text_generator.h5')
    transformer_model.model.save('transformer_text_generator.h5')

    # Save preprocessor
    with open('text_preprocessor.pkl', 'wb') as f:
        pickle.dump(preprocessor, f)

    print("Models and preprocessor saved successfully!")

    # Performance summary
    print("\n" + "="*80)
    print("MODEL PERFORMANCE SUMMARY")
    print("="*80)

    lstm_final_loss = history_lstm.history['val_loss'][-1]
    lstm_final_acc = history_lstm.history['val_accuracy'][-1]
    transformer_final_loss = history_transformer.history['val_loss'][-1]
    transformer_final_acc = history_transformer.history['val_accuracy'][-1]

    print(f"📊 LSTM Model:")
    print(f"   - Final Validation Loss: {lstm_final_loss:.4f}")
    print(f"   - Final Validation Accuracy: {lstm_final_acc:.4f}")
    print(f"   - Total Parameters: {lstm_model.model.count_params():,}")

    print(f"\n🤖 Transformer Model:")
    print(f"   - Final Validation Loss: {transformer_final_loss:.4f}")
    print(f"   - Final Validation Accuracy: {transformer_final_acc:.4f}")
    print(f"   - Total Parameters: {transformer_model.model.count_params():,}")

    print(f"\n📈 Dataset Statistics:")
    print(f"   - Vocabulary Size: {preprocessor.vocab_size:,}")
    print(f"   - Training Samples: {len(X_train):,}")
    print(f"   - Validation Samples: {len(X_val):,}")
    print(f"   - Sequence Length: {preprocessor.sequence_length}")

    print("\n" + "="*80)
    print("TEXT GENERATION PROJECT COMPLETED SUCCESSFULLY!")
    print("CODTECH INTERNSHIP - CERTIFICATE READY")
    print("="*80)

# Additional utility functions for advanced text generation
def generate_with_different_temperatures(model, prompt, preprocessor, temperatures=[0.5, 0.8, 1.0, 1.2]):
    """
    Generate text with different temperature settings

    Args:
        model: Trained model
        prompt (str): Input prompt
        preprocessor: Text preprocessor
        temperatures (list): List of temperature values
    """
    print(f"\n🌡️ Temperature Comparison for prompt: '{prompt}'")
    print("=" * 60)

    for temp in temperatures:
        generated = model.generate_text(prompt, preprocessor, max_length=100, temperature=temp)
        print(f"\n🌡️ Temperature {temp}:")
        print(f"{generated}")
        print("-" * 50)

def batch_generate_topics(model, preprocessor, topics, num_samples=3):
    """
    Generate multiple samples for different topics

    Args:
        model: Trained model
        preprocessor: Text preprocessor
        topics (list): List of topic prompts
        num_samples (int): Number of samples per topic

    Returns:
        dict: Dictionary mapping topics to generated texts
    """
    results = {}

    print(f"\n📚 Batch Generation for {len(topics)} topics")
    print("=" * 60)

    for topic in topics:
        print(f"\n🎯 Topic: {topic}")
        print("-" * 40)

        topic_results = []
        for i in range(num_samples):
            generated = model.generate_text(topic, preprocessor, max_length=120, temperature=0.9)
            topic_results.append(generated)
            print(f"Sample {i+1}: {generated[:100]}...")

        results[topic] = topic_results
        print()

    return results

def evaluate_text_quality(generated_texts, metric='diversity'):
    """
    Evaluate the quality of generated texts

    Args:
        generated_texts (list): List of generated text strings
        metric (str): Evaluation metric ('diversity', 'length', 'repetition')

    Returns:
        float: Quality score
    """
    if metric == 'diversity':
        # Calculate vocabulary diversity
        all_words = []
        for text in generated_texts:
            words = text.lower().split()
            all_words.extend(words)

        unique_words = len(set(all_words))
        total_words = len(all_words)

        return unique_words / total_words if total_words > 0 else 0.0

    elif metric == 'length':
        # Calculate average length
        lengths = [len(text.split()) for text in generated_texts]
        return sum(lengths) / len(lengths) if lengths else 0.0

    elif metric == 'repetition':
        # Calculate repetition score (lower is better)
        repetition_scores = []
        for text in generated_texts:
            words = text.lower().split()
            if len(words) <= 1:
                repetition_scores.append(0.0)
                continue

            bigrams = [(words[i], words[i+1]) for i in range(len(words)-1)]
            unique_bigrams = len(set(bigrams))
            total_bigrams = len(bigrams)

            repetition_score = 1.0 - (unique_bigrams / total_bigrams) if total_bigrams > 0 else 0.0
            repetition_scores.append(repetition_score)

        return sum(repetition_scores) / len(repetition_scores) if repetition_scores else 0.0

    else:
        raise ValueError(f"Unknown metric: {metric}")

def save_generated_samples(results, filename='generated_samples.txt'):
    """
    Save generated text samples to file

    Args:
        results (dict): Dictionary of generated texts by topic
        filename (str): Output filename
    """
    with open(filename, 'w', encoding='utf-8') as f:
        f.write("GENERATED TEXT SAMPLES\n")
        f.write("=" * 50 + "\n\n")

        for topic, samples in results.items():
            f.write(f"TOPIC: {topic}\n")
            f.write("-" * 30 + "\n")

            for i, sample in enumerate(samples, 1):
                f.write(f"Sample {i}:\n{sample}\n\n")

            f.write("\n" + "=" * 50 + "\n\n")

    print(f"Generated samples saved to {filename}")

def load_saved_models(lstm_path='lstm_text_generator.h5',
                     transformer_path='transformer_text_generator.h5',
                     preprocessor_path='text_preprocessor.pkl'):
    """
    Load saved models and preprocessor

    Args:
        lstm_path (str): Path to LSTM model
        transformer_path (str): Path to Transformer model
        preprocessor_path (str): Path to preprocessor

    Returns:
        tuple: (lstm_model, transformer_model, preprocessor)
    """
    try:
        # Load preprocessor
        with open(preprocessor_path, 'rb') as f:
            preprocessor = pickle.load(f)

        # Load LSTM model
        lstm_keras_model = tf.keras.models.load_model(lstm_path)
        lstm_model = LSTMTextGenerator(
            vocab_size=preprocessor.vocab_size,
            sequence_length=preprocessor.sequence_length
        )
        lstm_model.model = lstm_keras_model

        # Load Transformer model
        transformer_keras_model = tf.keras.models.load_model(transformer_path)
        transformer_model = SimpleTransformer(
            vocab_size=preprocessor.vocab_size,
            sequence_length=preprocessor.sequence_length
        )
        transformer_model.model = transformer_keras_model

        print("Models and preprocessor loaded successfully!")
        return lstm_model, transformer_model, preprocessor

    except FileNotFoundError as e:
        print(f"Error loading models: {e}")
        print("Please train the models first by running main()")
        return None, None, None

def advanced_text_generation_demo():
    """
    Demonstrate advanced text generation features
    """
    print("\n" + "="*80)
    print("ADVANCED TEXT GENERATION DEMO")
    print("="*80)

    # Try to load saved models
    lstm_model, transformer_model, preprocessor = load_saved_models()

    if lstm_model is None:
        print("No saved models found. Please run main() first to train the models.")
        return

    # Demo 1: Temperature comparison
    print("\n1. Temperature Effects on Generation")
    generate_with_different_temperatures(
        lstm_model,
        "Artificial intelligence will",
        preprocessor,
        temperatures=[0.3, 0.7, 1.0, 1.3]
    )

    # Demo 2: Batch topic generation
    print("\n2. Batch Topic Generation")
    demo_topics = [
        "Machine learning",
        "Climate change",
        "Space exploration",
        "Renewable energy",
        "Quantum physics"
    ]

    lstm_results = batch_generate_topics(lstm_model, preprocessor, demo_topics, num_samples=2)

    # Demo 3: Quality evaluation
    print("\n3. Text Quality Evaluation")
    all_generated = []
    for topic_samples in lstm_results.values():
        all_generated.extend(topic_samples)

    diversity_score = evaluate_text_quality(all_generated, 'diversity')
    avg_length = evaluate_text_quality(all_generated, 'length')
    repetition_score = evaluate_text_quality(all_generated, 'repetition')

    print(f"📊 Quality Metrics:")
    print(f"   - Vocabulary Diversity: {diversity_score:.3f}")
    print(f"   - Average Length: {avg_length:.1f} words")
    print(f"   - Repetition Score: {repetition_score:.3f} (lower is better)")

    # Demo 4: Save samples
    print("\n4. Saving Generated Samples")
    save_generated_samples(lstm_results, 'demo_generated_samples.txt')

    print("\nAdvanced demo completed!")

# Entry point for running the complete project
if __name__ == "__main__":
    # Run main training and demo
    main()

    # Run advanced features demo
    advanced_text_generation_demo()